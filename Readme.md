# SGFormer: Simplified Graph Transformers

The official implementation for NeurIPS23 paper "".

Related material: [[Paper]()]

SGFormer is a graph encoder backbone that efficiently computes all-pair interactions with one-layer attentive propagation.

SGFormer is built upon our previous works on scalable graph Transformers with linear complexity [NodeFormer](https://github.com/qitianwu/NodeFormer) (NeurIPS22, spotlight) and [DIFFormer](https://github.com/qitianwu/DIFFormer) (ICLR23, spotlight). 

## What's news

[2023.10.28] We release the code for the model on large graph benchmarks. More detailed info will be updated soon.

### Citation

If you find our codes useful, please cite our work. Thank you

```bibtex
      @inproceedings{
        wu2023sgformer,
        title={Simplifying and Empowering Transformers for Large-Graph Representations},
        author={Qitian Wu and Wentao Zhao and Chenxiao Yang and Hengrui Zhang and Fan Nie and Haitian Jiang and Yatao Bian and Junchi Yan},
        booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
        year={2023}
        }
```

